1) import numpy as np
import pandas as pd
2)data=pd.read_csv('C:/Users/aduttagu/Desktop/Msc-Thesis/LCL-FullData/CC_LCL-FullData.csv')
3)data.LCLid.unique()
4)data.describe()
5)data.info()
6)#### converting datatime
from datetime import datetime as dt 
format_str = '%Y-%m-%d %H:%M:%S.%f'
data['DateTime']=pd.to_datetime(data['DateTime'], format=format_str)
7)data.info()
8)data.columns = ['LCLid', 'stdorToU', 'DateTime', 'KWH/hh']
print(data.columns)
9)data.dropna(subset=["KWH/hh"], inplace=True)
10)print(data.isnull().sum())
11)### replacing null values with nan
data = data.replace('Null', np.nan)
12)## printing them
print(data.isnull().sum())
13)## dropping nan values
data.dropna(subset=["KWH/hh"], inplace=True)
14)## rechecking nan values
print(data.isnull().sum())
15)## making KWH/hh float type
data['KWH/hh'] = data['KWH/hh'].astype(float)
16)data.info()
17)## checking kwh avg per LCLid
avg_kwh = data.groupby('LCLid')['KWH/hh'].mean()
print(avg_kwh)
18)## checking min,max,mean value of avg
min_avgkwh = avg_kwh.min()
max_avgkwh = avg_kwh.max()
avg_avgkwh = avg_kwh.mean()
19)## checking min,max,mean value of avg
print(min_avgkwh, max_avgkwh, avg_avgkwh)
20)## checking min,max,mean value of avg (setting boundaries)
KWH_less_than_009 = (avg_kwh < 0.09).sum()
KWH_more_than_135 = (avg_kwh > 1.35).sum()

# print the results
print(KWH_less_than_009, KWH_more_than_135)
21)#transferring for data cleaning
data3 = data
22)# droping rows as per limits set
Yavg_kwh = data3.groupby('LCLid')['KWH/hh'].mean()

# drop rows where the average 'KWH/hh' is less than 0.09 or more than 1.35
to_drop = Yavg_kwh[(Yavg_kwh < 0.09) | (Yavg_kwh > 1.35)].index
data3 = data3[~data3['LCLid'].isin(to_drop)]

# reset the index of the dataframe
data3 = data3.reset_index(drop=True)
23)# counting readings
count_s = data3.groupby('LCLid')['KWH/hh'].count()
print(count_s)
24)# checking drop values existance
exists = "MAC000004" in data3['LCLid'].values
print(exists)
25)# checking grouped statistics
grouped_data3 = data3.groupby('LCLid')

# calculate the median, average, sum, highest, and lowest energy consumption for each 'LCLid'
median_consumption = grouped_data3['KWH/hh'].median()
average_consumption = grouped_data3['KWH/hh'].mean()
sum_consumption = grouped_data3['KWH/hh'].sum()
highest_consumption = grouped_data3['KWH/hh'].max()
lowest_consumption = grouped_data3['KWH/hh'].min()
26)# checking grouped statistics
# print the results
print('Energy consumption median:')
print(median_consumption)

print('Energy consumption average:')
print(average_consumption)

print('Energy consumption sum:')
print(sum_consumption)

print('Highest recorded energy consumption:')
print(highest_consumption)

print('Lowest recorded energy consumption:')
print(lowest_consumption)
27)# Adding days for better K-Means evaluation
data3['Days'] = data3['DateTime'].dt.dayofweek.map({0:1, 1:2, 2:3, 3:4, 4:5, 5:6, 6:7})
data3.info()
28) group_days = data3.groupby(['LCLid', 'Days'])['KWH/hh'].mean()

# find the weekday with the highest and lowest average energy consumption for each unique 'LCLid' group
highest_avg = group_days.groupby(level='LCLid').idxmax()
lowest_avg = group_days.groupby(level='LCLid').idxmin()

# print the result
print('Weekday with highest average energy consumption:')
print(highest_avg)
print('\nWeekday with lowest average energy consumption:')
print(lowest_avg)

29)unique_count = data3['LCLid'].nunique()
print("Number of unique LCLids:", unique_count)

30) #conversion for dividing the LCLID into 18 groups
highest_avg_values = np.array([tup[1] for tup in highest_avg.values], dtype=object)
lowest_avg_values = np.array([tup[1] for tup in lowest_avg.values], dtype=object)

31) # create a new dataframe with the required features
# create a new dataframe with the required features
kmeans_data = pd.DataFrame({
    'LCLid': grouped_data3['KWH/hh'].median().index,
    'median_consumption': median_consumption,
    'average_consumption': average_consumption,
    'sum_consumption': sum_consumption,
    'highest_consumption': highest_consumption,
    'lowest_consumption': lowest_consumption,
    'weekday with highest_avg': highest_avg_values,
    'weekday with lowest_avg': lowest_avg_values
})
32) kmeans_data

33) # standardize the data
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
kmeans_data_scaled = scaler.fit_transform(kmeans_data.iloc[:, 1:7])

34) # apply KMeans algorithm to divide the LCLIDs into 18 groups
from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=18, random_state=42)
kmeans.fit(kmeans_data_scaled)

35)# add cluster labels to the dataframe
kmeans_data['cluster'] = kmeans.labels_

# # print the number of LCLIDs in each cluster
print(kmeans_data['cluster'].value_counts())



36) #checking K-means data with cluster added
# reset the index of kmeans_data
kmeans_data = kmeans_data.reset_index(drop=True)
kmeans_data

37) Mergin the cluster number in the main data frame
# merge the dataframes based on LCLid
merged_data = pd.merge(data3, kmeans_data[['LCLid', 'cluster']], on='LCLid')

38) Initially, we proceed with all the data's in cluster 5
cluster_5_data = merged_data[merged_data['cluster'] == 5]
cluster_5_data

Federated learning
#1) All imports for federated learning and checking whether it is working with Hello World
import collections

import numpy as np
import tensorflow as tf
import tensorflow_federated as tff

np.random.seed(0)

tff.federated_computation(lambda: 'Hello, World!')()

2) # get the unique LCLid's in cluster 5

from sklearn.model_selection import train_test_split
unique_lclids_c5 = cluster_5_data['LCLid'].unique()

# split the unique LCLid's into training and test sets
train_lclids, test_lclids = train_test_split(unique_lclids_c5, test_size=0.2, random_state=42)

# filter the cluster 5 data based on the training and test LCLid's
train_data = cluster_5_data[cluster_5_data['LCLid'].isin(train_lclids)]
test_data = cluster_5_data[cluster_5_data['LCLid'].isin(test_lclids)]

# print the shapes of the training and test sets
print('Training data shape:', train_data.shape)
print('Test data shape:', test_data.shape)

3) # Define a function that returns the data for each client (i.e., unique LCLid)
def get_client_data():
    # Group the data by LCLid
    grouped_data = train_data.groupby('LCLid')

    # Create a dictionary mapping each LCLid to its corresponding data as a Pandas DataFrame
    client_data_dict = {}
    for client_id, client_data in grouped_data:
        client_data_dict[client_id] = client_data.drop('LCLid', axis=1)

    # Convert the dictionary to a list of (client_id, client_data) tuples
    client_data = [(client_id, client_data) for client_id, client_data in client_data_dict.items()]

    return tff.simulation.ClientData.from_clients_and_fn(
        client_data,
        lambda client_id: client_data_dict[client_id]
    )

# Define the input specification for the model
def get_input_spec():
    return train_data.drop(['LCLid', 'cluster', 'DateTime', 'rv1', 'rv2', 'day_of_week'], axis=1).iloc[0].shape.as_list()[0],

4) # Explore hetergeneosity like the google example

5) # Selecting data for the federated round
# Get the client data
client_data = get_client_data()
NUM_CLIENTS = 10 # Replace with desired number of clients
sample_clients = client_data.client_ids[0:NUM_CLIENTS]

def make_federated_data(client_data, client_ids):
    return [
        client_data.create_tf_dataset_for_client(x)
        for x in client_ids
    ]

federated_train_data = make_federated_data(client_data, sample_clients)

print(f'Number of client datasets: {len(federated_train_data)}')
print(f'First dataset: {federated_train_data[0]}')

6) # Define the model architecture
def create_model():
    model = tf.keras.Sequential([
        layers.Dense(64, activation='relu', input_shape=(4,)),
        layers.Dense(32, activation='relu'),
        layers.Dense(1)
    ])
    return model

7) #Defining federated avg process
def federated_averaging():
    # Define the model type
    model_type = tff.learning.from_keras_model(
        create_model(),
        input_spec=get_input_spec(),
        loss=tf.keras.losses.SparseCategoricalCrossentropy(),
        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]
    )

    # Define the federated averaging process
    return tff.learning.build_federated_averaging_process(
        model_fn=lambda: model_type,
        client_optimizer_fn=lambda: tf.keras.optimizers.SGD(lr=0.01),
        server_optimizer_fn=lambda: tf.keras.optimizers.SGD(lr=1.0),
        use_experimental_simulation_loop=True
    )
# Get the Federated Averaging process
federated_averaging_process = federated_averaging()

8) ###
Option 1 is an example of how to use the federated_averaging_process function provided by TensorFlow Federated (TFF) library to perform federated learning. 
In this option, the initialize() function is called to initialize the training state, and then the next() function is called multiple times in a loop 
to perform multiple rounds of federated learning. Each call to next() updates the training state and returns the updated state and training metrics for 
that round.

Option 2 is an example of how to implement federated learning from scratch using TFF library. In this option, a for loop is used to perform multiple 
rounds of federated learning. In each round, a subset of clients is selected, their data is retrieved using create_tf_dataset_for_client() function, 
and the local model training is performed using next() function, which returns the updated model state and metrics. Then, the updated model state is used 
for the next round of federated learning.

The main difference between these options is the level of abstraction. Option 1 uses the pre-implemented federated_averaging_process function, which 
abstracts away many of the details of federated learning, while option 2 implements the federated learning process from scratch, which gives more control 
over the details of the process. Additionally, option 1 assumes that the federated training data is already split into subsets for each client, while 
option 2 explicitly selects a subset of clients for each round of training


8.1) # Training the model

train_state = federated_averaging_process.initialize()
SERVER_STATE, FEDERATED_DATA -> SERVER_STATE, TRAINING_METRICS
result = federated_averaging_process.next(train_state, federated_train_data)
train_state = result.state
train_metrics = result.metrics
print('round  1, metrics={}'.format(train_metrics))

NUM_ROUNDS = 11
for round_num in range(2, NUM_ROUNDS):
  result = federated_averaging_process.next(train_state, federated_train_data)
  train_state = result.state
  train_metrics = result.metrics
  print('round {:2d}, metrics={}'.format(round_num, train_metrics))

8.2) # Run the simulation
state = federated_averaging_process.initialize()
for round_num in range(NUM_ROUNDS):
    # Sample clients for each round
    round_clients = np.random.choice(client_data.client_ids, size=NUM_CLIENTS_PER_ROUND, replace=False)
    # Get the data for the selected clients
    round_data = [client_data.create_tf_dataset_for_client(client_id) for client_id in round_clients]


    # Train the clients and get the updated model
    round_state, metrics = federated_averaging_process.next(state, round_data)

    # Print the metrics for this round
    print('Round {:2d}, metrics={}'.format(round_num, metrics))

    # Update the global state with the updated model
    state = round_state
   

9) # Evaluating the data against the test dataset
def get_test_client_data():
    # Group the data by LCLid
    grouped_test_data = test_data.groupby('LCLid')

    # Create a dictionary mapping each LCLid to its corresponding data as a Pandas DataFrame
    client_data_dict = {}
    for client_id, client_data in grouped_test_data:
        client_data_dict[client_id] = client_data.drop('LCLid', axis=1)

    # Convert the dictionary to a list of (client_id, client_data) tuples
    client_data = [(client_id, client_data) for client_id, client_data in client_data_dict.items()]

    return tff.simulation.ClientData.from_clients_and_fn(
        client_data,
        lambda client_id: client_data_dict[client_id]
    )

# Get the test client data
client_test_data = get_test_client_data()

# Get the test data for all clients
test_clients = client_test_data.client_ids
federated_test_data = make_federated_data(client_test_data, test_clients)

# Define a function to evaluate the model on the test data
@tff.tf_computation
def evaluate(model, test_data):
    # Initialize variables for computing metrics
    loss_sum = tf.Variable(0.0, dtype=tf.float32)
    num_examples = tf.Variable(0.0, dtype=tf.float32)

    # Iterate over the test data and compute loss and accuracy
    for batch in test_data:
        # Compute batch loss and accuracy
        batch_loss, batch_accuracy = model.evaluate(batch)
        
        # Update loss and accuracy metrics
        batch_size = tf.cast(tf.shape(batch['x'])[0], tf.float32)
        loss_sum.assign_add(batch_loss * batch_size)
        num_examples.assign_add(batch_size)

    # Compute final loss and accuracy metrics
    loss = loss_sum / num_examples
    accuracy = tf.constant(0.0, dtype=tf.float32)
    return {'loss': loss, 'accuracy': accuracy}

# Evaluate the model on the test data
test_metrics = evaluate(train_state.model, federated_test_data)
print('Test metrics:', test_metrics) 